{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ffd82d",
   "metadata": {},
   "source": [
    "# Training Data for Toxicity Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d16fb",
   "metadata": {},
   "source": [
    "This final was tough, taking a lot of time and trial and error before finding an algorithm that worked and worked fast. As a reminder, my initial goal for the program was to train a naïve Bayes classifier on a .csv of Wikipedia comments that can identify toxic or negative comments. Ultimately, the hope was that the program would see a potentially toxic comment and flag it for review. The proposed steps for solving this problem were the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab25b3ea",
   "metadata": {},
   "source": [
    "1. Build functions that parse through and normalize the .csv documents;\n",
    "2. Define a vocabulary of unique words within the normalized training data;\n",
    "3. Load training data into two arrays:\n",
    "4. An array of vectors based on the size of the vocabulary;\n",
    " - An array of labels for each comment in the training data;\n",
    " - Train the naive Bayes classifier with these arrays;\n",
    "5. Test the classifier on the test data;\n",
    "6. Calculate the prediction score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d136549",
   "metadata": {},
   "source": [
    "In practice, there were many changes to that initial proposal that had to be made in order to have a functioning program that did not take days to process. For instance, I did away with normalizing the data. Doing so took time to figure out, finding guidance in the article by Duque (2020). This program parsed through the original data, filtered and normalized it and saved that data to a new .txt file. While it worked, it took several hours to parse through and normalize all the data. Another roadblock I faced was in how we had learned to create a bag of words through a matrix of float tensors. Initially, I was going to use an rnn model like the ones created for labs 2 and 5. However, the bag of words turned out to be too large for my computer to handle, and the kernel would die regularly. As a result, I ended up having to do lots of research on quicker and easier means of parsing through and training the data. Those included the handbooks for pandas and sklearn, video tutorials and suggestions from others who completed the original Kaggle challenge. My final method for solving the problem was as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d91249",
   "metadata": {},
   "source": [
    "1. Import the modules necessary for reading csv files and multilabel classification training;\n",
    "2. Identify the training and testing data and normalize them;\n",
    "3. Use the scikit-learn vectorizing tools for making the data computer readable;\n",
    "4. Train the classifier and calculate its accuracy;\n",
    "5. Test the classifier;\n",
    "6. Save the tested classifications to a .csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4c157",
   "metadata": {},
   "source": [
    "The first step in solving this problem has to do with importing the modules necessary and creating a function that normalizes the text data. To begin, I imported pandas to help parse through the csv files and identify the relevant data. Then, I imported sklearn and some specific functions from that module used to training my program for multilabel identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37f393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # to read csv file\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9019353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "vect = TfidfVectorizer(stop_words = 'english')\n",
    "clf = OneVsRestClassifier(LogisticRegression(max_iter=3000)) # Uses OneVRest multilabel classification\n",
    "    # with logistic regression model as its parameters; 3000 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e86165",
   "metadata": {},
   "source": [
    "For the next step, I defined a function that parses through the training data and normalizes it. Doing so would hopefully make the training vectors a little less complicated, particularly in removing any unnecessary punctuation or complicated images ([NB-SVM strong linear baseline](href=https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline?scriptVersionId=2329069&cellId=1), [Duque (2020)](https://towardsdatascience.com/text-normalization-7ecc8e084e31))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9535d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, json\n",
    "def normalize_data(data):\n",
    "    contractions = json.loads(open('english_contractions.json').read())\n",
    "    data = data.lower()\n",
    "    data = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', '<URL>', data)\n",
    "    data = re.sub(r'#(\\w+)', '<HASHTAG>', data)\n",
    "    data = re.sub(r'@(\\w+)', '<MENTION>', data)\n",
    "    data = normalize_contractions(data, contractions)\n",
    "    data = re.sub(f'[{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’]', '', data)\n",
    "    data = re.sub(r'\\s+', ' ', data)\n",
    "    data = re.sub(r'\\d+', '', data)\n",
    "    return data\n",
    "\n",
    "def normalize_contractions(text, contractions):\n",
    "    new_token_list = []\n",
    "    token_list = text.split()\n",
    "    for word_pos in range(len(token_list)):\n",
    "        word = token_list[word_pos]\n",
    "        first_upper = False\n",
    "        if word[0].isupper():\n",
    "            first_upper = True\n",
    "        if word.lower() in contractions:\n",
    "            replacement = contractions[word.lower()]\n",
    "            if first_upper:\n",
    "                replacement = replacement[0].upper()+replacement[1:]\n",
    "            replacement_tokens = replacement.split()\n",
    "            if len(replacement_tokens) > 1:\n",
    "                new_token_list.append(replacement_tokens[0])\n",
    "                new_token_list.append(replacement_tokens[1])\n",
    "            else:\n",
    "                new_token_list.append(replacement_tokens[0])\n",
    "        else:\n",
    "            new_token_list.append(word)\n",
    "    sentence = \" \".join(new_token_list).strip(\" \")\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b2ae6",
   "metadata": {},
   "source": [
    "Now, I'm ready to establish my training set. Using the pandas.read_csv function, I establish which document is the training data and which one is the testing data. I also identified the list of tags used in the header of each file for later. Finally, I assign the normalized test and training data to variables X_trn and X_tst, respectively. Initially, I planned to do the same with the taglists, but I later realized that the data provided in __test_labels.csv__ does not accurately reflect the tags associated with __test.csv__. I am sure why they were included in the dataset, honestly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2116650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "#tst_tags = pd.read_csv('test_labels.csv')\n",
    "tagset=['toxic','severe_toxic','obscene','threat','insult','identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48677ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn = train.comment_text.apply(normalize_data)\n",
    "X_tst = test.comment_text.apply(normalize_data)\n",
    "Y_trn = train[tagset].values\n",
    "#Y_tst = test_tags[tagset].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877597a0",
   "metadata": {},
   "source": [
    "Using the data established above, I use TfidfVectorizer with the parameters I specified to create a matrix of my training data. Using the vocabulary established in fit_transform, I then create a matrix of the testing data. With the training matrix (trn_mtx), I use the OneVsRestClassifier with LogisticRegression to train on 3000 epochs. When it is complete, an accuracy will be recorded and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "588a8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_mtx = vect.fit_transform(X_trn)\n",
    "tst_mtx = vect.transform(X_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76b41005",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated accuracy is: 92.42%\n"
     ]
    }
   ],
   "source": [
    "clf.fit(trn_mtx, Y_trn)# Trains data classifier on training data\n",
    "accuracy = clf.score(trn_mtx, Y_trn) # Calculate the accuracy\n",
    "print('The calculated accuracy is: {:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a08bc",
   "metadata": {},
   "source": [
    "The final step in this process was to calculate predicted scores for the testing data and save those scores to an output file. Like I had said, I expected the data in __train_labels.csv__ to be more data to help calculate accuracy. However, the list of labels turned out to be -1's and 0's, with no information about the comments indicated. Upon further reflection, the data seems to have been added later for those competing to peruse. It was not initially part of the challenge, so I chose to ignore it and submit my predicted scores instead for perusal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68255809",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf.predict(tst_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cde70b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission[tagset] = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0eef2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('final_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea5b6b3",
   "metadata": {},
   "source": [
    "Like I said, this challenge was difficult and took a lot of extra research to complete. However, I found it very valuable as it taught me the logistics of teaching myself a machine learning software like scikit-learn and the established a solid foundation in multiclass output classification compared to the naïve Bayes binary classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
