{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ffd82d",
   "metadata": {},
   "source": [
    "# Training Data for Toxicity Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d136549",
   "metadata": {},
   "source": [
    "One of the major issues in working with the public is assessing when comments cross the line between playful banter or informative dialogue and toxicity. Under small-scale circumstances, a human might be able to manually flag and address toxicity on their own. However, large public forums like Twitter and Reddit do not have this privilege as the sheer number of posts and comments uploaded every day can be overwhelming for any human to do on their own.\n",
    "\n",
    "This then shows the importance of natural language processing for companies that offer public services like this. Utilizing the power of computers to identify patterns based on training data, data scientists can automate the process of flagging text for toxicity. That is the ultimate goal of this notebook: to develop a logistic regression model that can read text and flag it if it contains instances of toxicity, obscenities, threats, insults, or hateful language. After training the model, it will have a practical use of flagging unique data found in the test.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4c157",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "The first step in solving this problem is to import the useful modules and create a function that normalizes the text data. To begin, I imported pandas to help parse through the csv files and identify the relevant data. Then, I imported sklearn and some specific functions from that module used to training my program for multilabel identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b9019353",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd # to read csv file\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8982130",
   "metadata": {},
   "source": [
    "Next it's imperative to prepare the vectorizor and the model ahead of time. For this, I pulled the TfidfVectorizer which uses NLTK's word tokenizer method to identify tokens. It also filters out English stopwords as these aren't necessarily relevant. For the model, I have chosen the One vs Rest Classifier startegy which trains a logistic regression on multivariable data. The goal here is to flag the text for multiple types of toxicity when necessary, and this strategy allows for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "748df741",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words = 'english', token_pattern=\"<?\\w+>?\")\n",
    "clf = OneVsRestClassifier(LogisticRegression(max_iter=3000)) # Uses OneVRest multilabel classification\n",
    "    # with logistic regression model as its parameters; 1500 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e86165",
   "metadata": {},
   "source": [
    "## ETL for Language Data\n",
    "\n",
    "For the next step, I defined a function that transforms the training data and normalizes it through the power of vectorization. The function performs a series of normalization tasks through regular expressions and a function that seperates contractions. This will help ensure that the data is clean for the Tfidf Vectorizer so that urls, hashtags, mentions, and punctuation do not influence the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9535d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, json\n",
    "\n",
    "def normalize_data(data):\n",
    "    contractions = json.loads(open('english_contractions.json').read())\n",
    "    data = data.str.lower()\n",
    "    data = data.str.replace(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\n",
    "                            '<URL>',\n",
    "                            regex=True)\n",
    "    data = data.str.replace(r'#(\\w+)',\n",
    "                            '<HASHTAG>',\n",
    "                            regex=True)\n",
    "    data = data.str.replace(r'@(\\w+)',\n",
    "                            '<MENTION>',\n",
    "                            regex=True)\n",
    "    data = data.apply(lambda text: normalize_contractions(text, contractions))\n",
    "    punctuation = '['+re.sub('[<>]', '', string.punctuation) + '“”¨«»®´·º½¾¿¡§£₤‘’'+']'\n",
    "    data = data.str.replace(punctuation,\n",
    "                            '',\n",
    "                            regex=True)\n",
    "    data = data.str.replace('(<(?!=(URL|HASHTAG|MENTION))|(?!(URL|HASHTAG|MENTION))>)',\n",
    "                           '',\n",
    "                           regex=True)\n",
    "    data = data.str.replace(r'\\s+',\n",
    "                            ' ',\n",
    "                            regex=True)\n",
    "    data = data.str.replace(r'\\d+',\n",
    "                            '',\n",
    "                            regex=True)\n",
    "    return data\n",
    "\n",
    "def normalize_contractions(text, contractions):\n",
    "    new_token_list = []\n",
    "    token_list = text.split()\n",
    "    for word in token_list:\n",
    "        if word.lower() in contractions:\n",
    "            replacement = contractions[word.lower()]\n",
    "            replacement_tokens = replacement.split()\n",
    "            # contractions are now split, so each individual word needs to appended\n",
    "            if len(replacement_tokens) > 1:\n",
    "                new_token_list.append(replacement_tokens[0])\n",
    "                new_token_list.append(replacement_tokens[1])\n",
    "            else:\n",
    "                new_token_list.append(replacement_tokens[0])\n",
    "        else:\n",
    "            new_token_list.append(word)\n",
    "    sentence = \" \".join(new_token_list).strip(\" \")\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b2ae6",
   "metadata": {},
   "source": [
    "Now, I'm ready to establish my training set. Using the Pandas to read the csv, I download the training data from train.csv. I also identified the list of tags that will act as the dependent variables the model will train on. Next, I ensure the text data is normalized and ready for Tfidf vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2116650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "X = normalize_data(train_data['comment_text'])\n",
    "tagset=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "y = train_data[tagset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877597a0",
   "metadata": {},
   "source": [
    "Using the data established above, I use TfidfVectorizer with the parameters I specified to create a matrix of my training data. Using the vocabulary established in fit_transform, I then split the data into training and testing variables with sklearn's train_test_split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2000024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vect.fit_transform(X)\n",
    "X_trn, X_tst, y_trn, y_tst = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf9ac6",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "With the training matrix (trn_mtx), I use the OneVsRestClassifier with LogisticRegression to train on 3000 epochs. When it is complete, the accuracy score will be recorded and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "01b83521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneVsRestClassifier(estimator=LogisticRegression(max_iter=3000))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=LogisticRegression(max_iter=3000))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=3000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=3000)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(max_iter=3000))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_trn, y_trn) # Trains data classifier on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "76b41005",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated accuracy is: 91.64%\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_tst)\n",
    "\n",
    "accuracy = accuracy_score(y_tst, y_pred) # Calculate the accuracy\n",
    "print('The calculated accuracy is: {:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3a9f90",
   "metadata": {},
   "source": [
    "## Finding the Best Parameters\n",
    "\n",
    "While our current model is really accurate (between 91 and 92%), the question remains whether or not it is the most accurate we can get. To decide this, we must use a grid search to find the best parameters for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "6327c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'estimator__tol':[0.01, 0.001, 0.0001], 'estimator__max_iter':[1500,2500,3000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "11636155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.916384 using {'estimator__max_iter': 1500, 'estimator__tol': 0.001}\n",
      "Accuracy of the best classifier: 91.64%\n"
     ]
    }
   ],
   "source": [
    "#Instantiate the model\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=parameters, cv=5)\n",
    "\n",
    "# Fit grid_search to the data\n",
    "grid_search_result = grid_search.fit(X_trn, y_trn)\n",
    "\n",
    "# Summarize results\n",
    "best_score, best_params = grid_search_result.best_score_, grid_search_result.best_params_\n",
    "print(\"Best: %f using %s\" % (best_score, best_params))\n",
    "\n",
    "# Extract the best model and evaluate it on the test set\n",
    "best_model = grid_search_result.best_estimator_\n",
    "print(\"Accuracy of the best classifier: {:.2%}\".format(best_model.score(X_tst, y_tst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a08bc",
   "metadata": {},
   "source": [
    "## Practical Usage\n",
    "\n",
    "With the best model selected and trained, I am ready to move to the final step. Here, I take the test.csv file which only contains the comments that have yet to be tagged. The process will largely remain the same, with the difference being a lack of an accuracy score. I first normalize and vectorize the data, then have the model predict toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "911fd5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = pd.read_csv('test.csv').set_index(keys=['id'])\n",
    "test_data = normalize_data(prediction_data['comment_text'])\n",
    "tst_mtx = vect.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "68255809",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = best_model.predict(tst_mtx)\n",
    "prediction_data[tagset] = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b46395",
   "metadata": {},
   "source": [
    "Out of curiousity, I also single out the indexes where the comments were deemed toxic, and print the first 10 to see which comments these were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "64be2046",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\n",
      "\n",
      "===============New Comment===============\n",
      ":Dear god this site is horrible.\n",
      "\n",
      "===============New Comment===============\n",
      "\" \n",
      "\n",
      " ==balance== \n",
      " This page has one sentence about the basic definition of the word, and a huge amount about the slang/profane uses. Perhaps the former should be extended; is there no information about female dogs available beyond their name? This is an encyclopaedia, not a dictionary.  \n",
      "\n",
      "  \n",
      " i feel that whoever is looking this definition up is very appropiate and should be deleted from wikipedia...IMMEDIATLY. this word is used very often and is also a very \"\"mean\"\" word. i belive that is majorly true. very much so. okay so, the good meaning is a female dog.  BITCH !!!!!!!!!It also stands for the name Brittany Fellows—Preceding unsigned comment added by   •    \n",
      "\n",
      " ==etymology== \n",
      " The word bitch is from the Old Norse Bikkjuna meaning \"\"female of the dog\"\" of unknown origin, Grimm derives the Old Norse words from Lapp Pittja, But OED notes that \"\"the converse is equally possible.\"\" \n",
      " The adj. Bitchy was first seen in 1925.  \n",
      " The verb meaning to complain in 1930. \n",
      " Slang Bitchen \"\"good\"\" is attested to the 1950's \n",
      "\n",
      " == reclaiming the word bitch == \n",
      " The word bitch is actually only offencive in American and Canadian English. In most other English variants bitch maintains it's correct definition - female canine. People have argued that bitch is different because it is used as an insult - but so is \"\"pig\"\", \"\"dog\"\", \"\"cow\"\" and others. These are not considered profane, so why is bitch? As far as I am concerned, there is nothing to \"\"reclaim\"\" as bitch simply means a female canine. \n",
      "\n",
      " It may be used as a pejorative or descriptor - that does not make it a profanity. \n",
      "\n",
      "    \n",
      " It should definately say something about Kyle Vanderweilen and all his bitchin Is there any particular evidence of women \"\"reclaiming\"\" the word bitch in the 90s? Can anyone point to articles on this, etc.? The song is definitely interesting and belongs here, but doesn't actually reclaim the word \"\"bitch\"\" any more than it reclaims the word \"\"sinner.\"\" \n",
      "\n",
      " Also, I don't really understand the last paragraph and it sucks  I was going to try and clear it up, but I realized I don't know what it means. Can someone point to a source that lays out the argument about bitches, fertility and patriarchy more clearly? \n",
      "\n",
      " \n",
      "\n",
      " :We don't need articles at all as I there are definitely enough examples (even outside of the 90s). Missy Elliot cleary and repeatedly reclaims the word, for instance, \"\"She's a Bitch\"\".   \n",
      "\n",
      " ::References are fine  just the name \"\"Missy Elliot\"\" got me to find a quote of a rolling stone review that mentioned \"\"reclaiming\"\" the word. In fact, if some one knows more about her, it might be interesting to add a section on her work and on how it relates to women/slurs more generally within hip-hop.   \n",
      "\n",
      " :::How relevant is this reclamation? It seems to me poison for a woman to use an epithet which still strongly connotes despised traits. I don't see how using a word connoting \"\"querolous\"\", \"\"spiteful\"\", and \"\"malicious\"\" can be empowering. But I don't know; life's a bitch.   \n",
      "\n",
      " ::::It stems from a reaction to the prevalence of tagging any woman who doesn't adhere to a certain standard of femininity as a bitch. To use a literary example, in the novel The Handmaid's Tale the narrator related the feeling that, whenever she outsmarted a man, she could almost hear him calling her a bitch in his mind, even her own husband. -   \n",
      "\n",
      " :::::That literary reaction you recited is jealousy; men have names for other men who outsmart them too, but, not having a word like \"\"bitch\"\" for them, they have to resort to a greater variety of pejoritive epithets. Though you haven't been specific, I think that standard of femininity you referred to is a pretext for subordination, which both sexes often strive to impose on the other, but men have traditionally had more power. Because it is such a popular epithet in slang, the connotations of \"\"bitch\"\" are diffuse, though still commonly pernicious. I think bitch, in itself, is not chiefly a denunciation to punish women who do not conform to that standard, but rather a contemptuous word often used for it. \n",
      "\n",
      " :::::However, I still don't see the point of reclamati\n",
      "\n",
      "===============New Comment===============\n",
      "DJ Robinson is gay as hell! he sucks his dick so much!!!!!\n",
      "\n",
      "===============New Comment===============\n",
      ":Fuck off, you anti-semitic cunt.  |\n",
      "\n",
      "===============New Comment===============\n",
      "== Hello == \n",
      "\n",
      " Fuck off my Pagan you bareback mancunt pig slut.\n",
      "\n",
      "===============New Comment===============\n",
      "\" August 2006 (UTC) \n",
      "\n",
      " :::::A simple \"\"you're wrong\"\" would have sufficed.No need to try and make me look like an idiot 13:01, 10\"\n",
      "\n",
      "===============New Comment===============\n",
      "::No, he is an arrogant, self serving, immature idiot. Get it right.\n",
      "\n",
      "===============New Comment===============\n",
      "::You're funny.  Ugly?  We're dudes on computers, moron.  You are quite astonishingly stupid.\n",
      "\n",
      "===============New Comment===============\n",
      "Stone Sour sucks anus\n"
     ]
    }
   ],
   "source": [
    "toxic_ids = prediction_data[tagset].where(prediction_data[tagset] == 1).dropna(how='all')\n",
    "toxic_comments = prediction_data[prediction_data.index.isin(toxic_ids.index)]['comment_text'].tolist()\n",
    "print(*toxic_comments[:10], sep='\\n\\n===============New Comment===============\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f8b8f0",
   "metadata": {},
   "source": [
    "Finally, I save the output of the practical usage into a csv, where it can be accessed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e0eef2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data.to_csv('final_output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
